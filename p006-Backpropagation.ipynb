{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Code Backpropagation__\n",
    "\n",
    "Before applying propagation to our code, let's understand and implement it with a basic example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [1, -2, 3]\n",
    "x = [-3, -1, 2]\n",
    "b = 1\n",
    "\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "y = max(0, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implemented a basic layer with relu activation. Let's take a look at the neuron's function\n",
    "\n",
    "$f(x) = ReLu(sum(mul(x0, w0), mul(x1, w1), mul(x2, w2), b))$\n",
    "\n",
    "Let's we want to find the impact of $x_{0}$ on this function. It would look like\n",
    "\n",
    "#### $\\frac{\\partial}{\\partial x_(0)}f(x) = \\frac{dReLu()}{dsum()} * \\frac{dsum()}{dmul()} * \\frac{dmul()}{x_{0}}$\n",
    "\n",
    "By implementing the chain rule, we can now compute the impact of $x_{0}$ on this function. While we calculate backward pass, we will be recieving gradients from previous layers, which we can set to 1 in our example. To calculate the derivative of relu with respect to derivative of sum, will just be 1, since we know that the sum function has an output more than 0, which is then multip;ed by gradient from next layer, with respect to current, so that we follow the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvalue = 1\n",
    "drelu_dsum = (1 if z > 0 else 0) * dvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the derivative of sum with respect to weighted inputs, which is always 1, since partial derivative of sum is always 1. Using this, we can get the derivative of relu, with respect to weighted inputs. We can also define derivative of sum function with respect to bias is also 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "drelu_dxw0 = dsum_dxw0 * drelu_dsum\n",
    "drelu_dxw1 = dsum_dxw1 * drelu_dsum\n",
    "drelu_dxw2 = dsum_dxw2 * drelu_dsum\n",
    "drelu_db = dsum_db * drelu_dsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the multiplication rule of partial derivatives to first get derivative of multiplication function with respect to inputs, which is just the weights, then get the rest of the derivative and multiply to finish! We can also get the impact of wieghts, by using multiplication rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmul_dx0 = w[0]\n",
    "dmul_dx1 = w[1]\n",
    "dmul_dx2 = w[2]\n",
    "dmul_dw0 = x[0]\n",
    "dmul_dw1 = x[1]\n",
    "dmul_dw2 = x[2]\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0\n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1\n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1\n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2\n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can represent these as gradients!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2]\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2]\n",
    "db = drelu_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement our findings into more realistic code, before incorporating it with our main code. All we are doing here, is taking we what we did in the last few lines, and apply it to more realistic data and paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.1  2.5  0.6  0.5]\n",
      " [ 2.6 -3.   2.  -2.2]\n",
      " [-3.9  4.5 -3.   3.3]]\n",
      "[[  9.5  -7.5   7.5]\n",
      " [  3.9   0.1  -0.1]\n",
      " [ -8.9  14.9 -14.9]\n",
      " [  8.9  -3.9   3.9]]\n",
      "[[ 0  2 -2]]\n",
      "[[1 1 0]\n",
      " [2 0 2]\n",
      " [0 3 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dvalues = np.array([[1, 1, -1], [2, -2, 2], [-3, 3, -3]])\n",
    "weights = np.array([[0.1, 0.5, 0.8, -0.3],\n",
    "                   [-0.7, 1.2, -0.9, 1.1],\n",
    "                   [0.5, -0.8, -0.7, 0.3]]).T\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                   [2, 5, -1, 2],\n",
    "                   [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "dinputs = np.dot(dvalues, weights.T)\n",
    "print(dinputs)\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "print(dweights)\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "print(dbiases)\n",
    "drelu = np.zeros_like(dvalues)\n",
    "drelu[dvalues > 0] = 1\n",
    "drelu *= dvalues\n",
    "print(drelu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to add everything to our original code. Then we will learn to compute the gradients of the softmax and loss functions. After that we will apply optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
