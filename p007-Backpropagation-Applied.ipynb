{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Applying Backpropagation__\n",
    "Let's first start by editing the Dense class. We can just use what we in the previous notebook, and apply it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "  def __init__(self, input_neurons, output_neurons):\n",
    "    self.weights = 0.1*np.random.randn(input_neurons, output_neurons)\n",
    "    self.biases = np.zeros((1, output_neurons))\n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    self.output = np.dot(inputs, self.weights) + self.biases\n",
    "  def backprop(self, dvalues):\n",
    "    self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "    self.dbiases = np.sum(dvalues, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's edit the relu class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu:\n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    self.output = np.maximum(0, inputs)\n",
    "  def backprop(self, dvalues):\n",
    "    self.dinputs = dvalues.copy()\n",
    "    self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, in both classes we added a variable called inputs, which is saved while we do the forward pass. This is because we need those inputs, when we are calculating our gradients during backpropagation. But what about the softmax and loss function derivatives. Well we can actually combine them together, which will also make our gradient calculations easier and faster. Let's start by taking the partial derivative of the loss function with respect to the inputs of the softmax function. We can use the chain rule to do this.\n",
    "\n",
    "### $\\frac{\\partial L_{i}}{\\partial z_{i,k}}$\n",
    "\n",
    "This represent the gradient of the loss $L_{i}$ with respect to the input $z_{i,k}$ of the softmax function from class $k$. Now we can apply chain rule \n",
    "\n",
    "### $\\frac{\\partial L_{i}}{\\partial z_{i,k}} = \\frac{\\partial L_{i}}{\\partial \\hat y_{i,k}}\\times\\frac{\\partial \\hat y_{i,k}}{\\partial z_{i,k}}$\n",
    "\n",
    "Now we have the gradient of the loss function, with respect to the softmax output, times the gradient of the softmax output, with respect to the input. The gradients are:\n",
    "\n",
    "### $\\frac{\\partial L_{i}}{\\partial \\hat y_{i,k}} = - \\frac {y_{i,j}}{\\hat y_{i,j}}$\n",
    "Calculated using partial derivative of multiplication\n",
    "\n",
    "Now there are two possibilities of the softmax gradient. The reason is because since $j$ and $k$ both represent classes, either $j=k$ or $j \\neq k$\n",
    "\n",
    "### $\\frac{\\partial \\hat y_{i,k}}{\\partial z_{i,k}} = \\hat y_{i, k}(1 - \\hat y_{i, k}), (j = k)$\n",
    "\n",
    "### $\\frac{\\partial \\hat y_{i,k}}{\\partial z_{i,k}} = -\\hat y_{i, k} \\times \\hat y_{i, j}, (j \\neq k)$\n",
    "\n",
    "This helps us get what impact the input has on its class and other classes. By combining both the gradient, we our solution.\n",
    "\n",
    "### $\\frac{\\partial L_{i}}{\\partial z_{i,k}} = \\hat y_{i, k} - y_{i, k}$\n",
    "\n",
    "In simpler terms, the gradient for loss function, with respect to softmax, is the predicted minus the true probability. This is simple to apply, because for the ones that aren't the true, we can leave them unchanged since their true probability is 0, but for the ones that are correct, we can just subtract 1, meaning they are the true label. We set up a combined class, initialize our variables, and add a backprop method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "  def forward(self, inputs):\n",
    "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "    self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "class Loss:\n",
    "  def calculate(self, y_pred, y_true):\n",
    "    samples = len(y_pred)\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "    correct_confidences = y_pred[range(samples), y_true]\n",
    "    return -np.mean(np.log(correct_confidences))\n",
    "\n",
    "class Softmax_Loss:\n",
    "  def __init__(self):\n",
    "    self.activation = Softmax()\n",
    "    self.loss = Loss()\n",
    "  def forward(self, inputs, y_true):\n",
    "    self.activation.forward(inputs)\n",
    "    self.output = self.activation.output\n",
    "    return self.loss.calculate(self.output, y_true)\n",
    "  def backprop(self, dvalues, y_true):\n",
    "    samples = len(dvalues)\n",
    "    self.dinputs = dvalues.copy()\n",
    "    self.dinputs[range(samples), y_true] -= 1\n",
    "    self.dinputs /= samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Training Model__\n",
    "\n",
    "We are finally finished with backpropagation, and can start training our model. We will start by getting our dataset, and defining our neural network. Then we will set some hyperparameters, called\n",
    "\n",
    "1. Epochs - How many times will the model train using the training data\n",
    "2. Batch Size - How many samples will the model look at once\n",
    "3. Learning Rate - How much of the gradient we will use to update\n",
    "\n",
    "After that we will create a training loop, which will run based on number of epochs. Then we will shuffl our data, and create another loop. This loop will process the data batch by batch. First we will create batches. Then we will perform forward pass, and then backpropagation. After that we will update our weights and biases based on our learning rate and gradients, and display our loss and accuracy, which we will calculate. Then after training, we will try our model on a test dataset, and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.66908, Accuracy: 75.00%\n",
      "Epoch 2/20, Loss: 0.92674, Accuracy: 78.12%\n",
      "Epoch 3/20, Loss: 0.38734, Accuracy: 90.62%\n",
      "Epoch 4/20, Loss: 0.75978, Accuracy: 87.50%\n",
      "Epoch 5/20, Loss: 0.27943, Accuracy: 90.62%\n",
      "Epoch 6/20, Loss: 0.32375, Accuracy: 90.62%\n",
      "Epoch 7/20, Loss: 0.65538, Accuracy: 81.25%\n",
      "Epoch 8/20, Loss: 0.16089, Accuracy: 96.88%\n",
      "Epoch 9/20, Loss: 0.64477, Accuracy: 84.38%\n",
      "Epoch 10/20, Loss: 0.27178, Accuracy: 96.88%\n",
      "Epoch 11/20, Loss: 0.14102, Accuracy: 93.75%\n",
      "Epoch 12/20, Loss: 0.45884, Accuracy: 84.38%\n",
      "Epoch 13/20, Loss: 0.18597, Accuracy: 93.75%\n",
      "Epoch 14/20, Loss: 0.25083, Accuracy: 93.75%\n",
      "Epoch 15/20, Loss: 0.09507, Accuracy: 100.00%\n",
      "Epoch 16/20, Loss: 0.22362, Accuracy: 96.88%\n",
      "Epoch 17/20, Loss: 0.20952, Accuracy: 93.75%\n",
      "Epoch 18/20, Loss: 0.03129, Accuracy: 100.00%\n",
      "Epoch 19/20, Loss: 0.02739, Accuracy: 100.00%\n",
      "Epoch 20/20, Loss: 0.08755, Accuracy: 96.88%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "with open('dataset.p', 'rb') as file:\n",
    "  X, y = pickle.load(file)\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "layer_one = Dense(784, 128)\n",
    "relu_one = ReLu()\n",
    "layer_two = Dense(128, 64)\n",
    "relu_two = ReLu()\n",
    "layer_three = Dense(64, 10)\n",
    "softmax_loss = Softmax_Loss()\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  indices = np.arange(len(X_train))\n",
    "  np.random.shuffle(indices)\n",
    "  X_train, y_train = X_train[indices], y_train[indices]\n",
    "  \n",
    "  for i in range(0, len(X_train), batch_size):\n",
    "    X_batch, y_batch = X_train[i:i+batch_size], y_train[i:i+batch_size]\n",
    "     \n",
    "    layer_one.forward(X_batch)\n",
    "    relu_one.forward(layer_one.output)\n",
    "    layer_two.forward(relu_one.output)\n",
    "    relu_two.forward(layer_two.output)\n",
    "    layer_three.forward(relu_two.output)\n",
    "    loss = softmax_loss.forward(layer_three.output, y_batch)\n",
    "\n",
    "    predictions = np.argmax(softmax_loss.output, axis=1)\n",
    "    accuracy = np.mean(predictions == y_batch)\n",
    "\n",
    "    softmax_loss.backprop(softmax_loss.output, y_batch)\n",
    "    layer_three.backprop(softmax_loss.dinputs)\n",
    "    relu_two.backprop(layer_three.dinputs)\n",
    "    layer_two.backprop(relu_two.dinputs)\n",
    "    relu_one.backprop(layer_two.dinputs)\n",
    "    layer_one.backprop(relu_one.dinputs)\n",
    "\n",
    "    layer_one.weights -= learning_rate * layer_one.dweights\n",
    "    layer_one.biases -= learning_rate * layer_one.dbiases\n",
    "    layer_two.weights -= learning_rate * layer_two.dweights\n",
    "    layer_two.biases -= learning_rate * layer_two.dbiases\n",
    "    layer_three.weights -= learning_rate * layer_three.dweights\n",
    "    layer_three.biases -= learning_rate * layer_three.dbiases\n",
    "\n",
    "  print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.5f}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, it looks like our model did a great job, with a high accuracy on the training data. Let's see how it does on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.25982\n",
      "Test Accuracy: 93.07%\n"
     ]
    }
   ],
   "source": [
    "layer_one.forward(X_test)\n",
    "relu_one.forward(layer_one.output)\n",
    "layer_two.forward(relu_one.output)\n",
    "relu_two.forward(layer_two.output)\n",
    "layer_three.forward(relu_two.output)\n",
    "\n",
    "test_loss = softmax_loss.forward(layer_three.output, y_test)\n",
    "print(f'Test Loss: {test_loss:.5f}') \n",
    "\n",
    "predictions = np.argmax(softmax_loss.output, axis=1) \n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the testing accuracy is not as high as we would like it to be. Not to worry, as we will next cover optimizers and how they can really make our learning process, faster and better for the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
