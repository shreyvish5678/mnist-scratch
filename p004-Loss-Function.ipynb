{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Loss Function__\n",
    "\n",
    "Let's get our code from the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dense:\n",
    "  def __init__(self, input_neurons, output_neurons):\n",
    "    self.weights = np.random.randn(input_neurons, output_neurons)\n",
    "    self.biases = np.zeros((1, output_neurons))\n",
    "  def forward(self, inputs):\n",
    "    self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class ReLu:\n",
    "  def forward(self, inputs):\n",
    "    self.output = np.maximum(0, inputs)\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)   \n",
    "\n",
    "layer_one = Dense(784, 128)\n",
    "layer_two = Dense(128, 64)\n",
    "layer_three = Dense(64, 10)\n",
    "\n",
    "relu_one = ReLu()\n",
    "relu_two = ReLu()\n",
    "softmax = Softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's learn how loss is calculated. Consider a ball in a picture. It can be either red, green or blue. A sample model will produce a probability distribution of the probabilities, such as $\\hat{y}=[0.7, 0.1, 0.2]$, where the values represent green, red, and blue respectively. The reason we are using probability distribtuions instead of accuracy, is that along with telling us about if our mode made incorrect predictions, it tells us how incorrect were they, or it's also done to increase the confidence of the model. Now the label of that picture was green, which can be represented with a probability distribution of $y=[1, 0, 0]$. To calculate loss for each sample, we first calculate the log of the values in $\\hat{y}$, and then we multiply them by the corresponding true value, then multiply by -1. After that we add it all up, which looks like:\n",
    "\n",
    "$L = -(1 \\times \\ln(0.7) + 0 \\times \\ln(0.2) + 0 \\times \\ln(0.1)) = -0.3563$\n",
    "\n",
    "### Loss function: $-\\sum_{ }^{ }y_{i}\\cdot\\ln\\left(\\hat{y_{i}}\\right)$\n",
    "\n",
    "To simplify the loss of a sample, is the negative of the natural log of the true value. Now let's implement loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "  def calculate(self, y_pred, y_true):\n",
    "    samples = len(y_pred)\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "    correct_confidences = y_pred[range(samples), y_true]\n",
    "    return -np.mean(np.log(correct_confidences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get how many samples are in a batch, so we can go through each sample, and get the correct confidence for each. We clip the predicted values, because if for the correct label they are 0, we can recieve log errors. Then using some indexing we extract the correct confidences, and use our loss function to calculate the loss for the entire batch, which is then returned. Now let's import the dataset and implement our new code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.53908326617032\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('dataset.p', 'rb') as file:\n",
    "  X, y = pickle.load(file)\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "loss_function = Loss()\n",
    "\n",
    "layer_one.forward(X)\n",
    "relu_one.forward(layer_one.output)\n",
    "layer_two.forward(relu_one.output)\n",
    "relu_two.forward(layer_two.output)\n",
    "layer_three.forward(relu_two.output)\n",
    "softmax.forward(layer_three.output)\n",
    "loss = loss_function.calculate(softmax.output, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the loss is around 14. This will obviously be the case, since we have not actually trained our model. But that's what we will do next time, when we implement backpropagation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
