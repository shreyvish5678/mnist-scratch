{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-14T17:12:55.698402Z","iopub.status.busy":"2023-08-14T17:12:55.697931Z","iopub.status.idle":"2023-08-14T17:12:55.704236Z","shell.execute_reply":"2023-08-14T17:12:55.703101Z","shell.execute_reply.started":"2023-08-14T17:12:55.698367Z"},"trusted":true},"outputs":[],"source":["#import libraries\n","import numpy as np\n","import pickle"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-14T17:12:55.724000Z","iopub.status.busy":"2023-08-14T17:12:55.723628Z","iopub.status.idle":"2023-08-14T17:12:59.759090Z","shell.execute_reply":"2023-08-14T17:12:59.757856Z","shell.execute_reply.started":"2023-08-14T17:12:55.723972Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train features: (60000, 784), labels: (60000,)\n","Test features: (10000, 784), labels: (10000,)\n"]}],"source":["#read dataset\n","with open('train.p', 'rb') as f:\n","    X_train, y_train = pickle.load(f)\n","with open('test.p', 'rb') as f:\n","  X_test, y_test = pickle.load(f)\n","#flatten image arrays into vectors\n","X_train = X_train.reshape(X_train.shape[0], -1)\n","X_test = X_test.reshape(X_test.shape[0], -1)\n","#normalize features\n","X_train = X_train.astype('float32') / 255.0\n","X_test = X_test.astype('float32') / 255.0\n","#split into training and testing\n","print(f\"Train features: {X_train.shape}, labels: {y_train.shape}\")\n","print(f\"Test features: {X_test.shape}, labels: {y_test.shape}\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-08-14T17:12:59.762413Z","iopub.status.busy":"2023-08-14T17:12:59.761972Z","iopub.status.idle":"2023-08-14T17:12:59.781282Z","shell.execute_reply":"2023-08-14T17:12:59.780269Z","shell.execute_reply.started":"2023-08-14T17:12:59.762373Z"},"trusted":true},"outputs":[],"source":["#create dense layer class\n","class Dense:\n","    def __init__(self, input_neurons, output_neurons):\n","        #generate weights and biases, based on given params\n","        self.weights = 0.1*np.random.randn(input_neurons, output_neurons)\n","        self.biases = np.zeros((1, output_neurons))\n","    def forward(self, inputs):\n","        #store inputs for backprop\n","        self.inputs = inputs\n","        #calculate output\n","        self.output = np.dot(inputs, self.weights) + self.biases\n","    def backprop(self, dvalues):\n","        #get gradients for weights, inputs and biases by implementing calculus\n","        self.dinputs = np.dot(dvalues, self.weights.T)\n","        self.dweights = np.dot(self.inputs.T, dvalues)\n","        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n","        \n","\n","#create relu activation class\n","class ReLu:\n","    def forward(self, inputs):\n","        #store inputs for backprop\n","        self.inputs = inputs\n","        #apply relu activation\n","        self.output = np.maximum(0, inputs)\n","    def backprop(self, dvalues):\n","        self.dinputs = dvalues.copy()\n","        #apply relu derivative\n","        self.dinputs[self.inputs <= 0] = 0\n","\n","#create softmax activation class\n","class Softmax:\n","    def forward(self, inputs):\n","        #subtract max of each sample from each value and perform exponentation\n","        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n","        #divide by sum of sample, to create probability distribution\n","        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n","    \n","#create loss class\n","class Loss_Categorical_Crossentropy:\n","    def calculate(self, y_pred, y_true):\n","        #get number of samples in batch\n","        samples = len(y_pred)\n","        #clip to avoid log errors\n","        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n","        #get correct confidences\n","        correct_confidences = y_pred[range(samples), y_true]\n","        #return loss for each sample, using negative log\n","        return -np.mean(np.log(correct_confidences))\n","\n","#create combined softmax CC class, for ease of backpropagation\n","class Softmax_Loss_CC:\n","    def __init__(self):\n","        #def initializer method\n","        self.activation = Softmax()\n","        self.loss = Loss_Categorical_Crossentropy()\n","    def forward(self, inputs, y_true):\n","        #do softmax activation\n","        self.activation.forward(inputs)\n","        self.output = self.activation.output\n","        #return loss from softmax\n","        return self.loss.calculate(self.output, y_true)\n","    def backprop(self, dvalues, y_true):\n","        #get number of samples in previous gradient\n","        samples = len(dvalues)\n","        self.dinputs = dvalues.copy()\n","        #apply softmax_loss_CC derivative to calculate gradient\n","        self.dinputs[range(samples), y_true] -= 1\n","        #normalize gradient\n","        self.dinputs /= samples         "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-08-14T17:12:59.783760Z","iopub.status.busy":"2023-08-14T17:12:59.783054Z","iopub.status.idle":"2023-08-14T17:12:59.813025Z","shell.execute_reply":"2023-08-14T17:12:59.812118Z","shell.execute_reply.started":"2023-08-14T17:12:59.783727Z"},"trusted":true},"outputs":[],"source":["#define adam optimizer class\n","class Optimizer_Adam:\n","    #set parameters and iterations\n","    def __init__(self, learning_rate=0.001, epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n","        self.learning_rate = learning_rate\n","        self.epsilon = epsilon\n","        self.beta_1 = beta_1\n","        self.beta_2 = beta_2\n","        self.iterations = 0\n","\n","    def update_params(self, layer):\n","        if not hasattr(layer, 'weight_cache'):\n","            #initialize momentums and cache\n","            layer.weight_momentums = np.zeros_like(layer.weights)\n","            layer.weight_cache = np.zeros_like(layer.weights)\n","            layer.bias_momentums = np.zeros_like(layer.biases)\n","            layer.bias_cache = np.zeros_like(layer.biases)\n","            \n","        #update momentums using beta_1\n","        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n","        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n","        \n","        #bias correction for momentums\n","        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n","        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n","        \n","        #update cache using beta_2\n","        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n","        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n","        \n","        #bias correction for cache\n","        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n","        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n","        \n","        #update weights and biases\n","        layer.weights += -self.learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n","        layer.biases += -self.learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n","\n","    def post_update_params(self):\n","        #update iterations\n","        self.iterations += 1"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-08-14T17:12:59.815154Z","iopub.status.busy":"2023-08-14T17:12:59.814779Z","iopub.status.idle":"2023-08-14T17:12:59.834597Z","shell.execute_reply":"2023-08-14T17:12:59.833090Z","shell.execute_reply.started":"2023-08-14T17:12:59.815124Z"},"trusted":true},"outputs":[],"source":["#define neural net architecture\n","dense1 = Dense(784, 128)\n","relu1 = ReLu()\n","dense2 = Dense(128, 64)\n","relu2 = ReLu()\n","dense3 = Dense(64, 10)\n","#define softmax_loss instance and optimizer\n","softmax_loss = Softmax_Loss_CC()\n","optimizer = Optimizer_Adam()\n","#define hyperparams\n","epochs = 5\n","batch_size = 32"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-08-14T17:12:59.839257Z","iopub.status.busy":"2023-08-14T17:12:59.838335Z","iopub.status.idle":"2023-08-14T17:13:29.316922Z","shell.execute_reply":"2023-08-14T17:13:29.315433Z","shell.execute_reply.started":"2023-08-14T17:12:59.839192Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5, loss:0.28508, accuracy: 90.625%\n","Epoch 2/5, loss:0.00719, accuracy: 100.000%\n","Epoch 3/5, loss:0.08206, accuracy: 96.875%\n","Epoch 4/5, loss:0.02417, accuracy: 100.000%\n","Epoch 5/5, loss:0.00398, accuracy: 100.000%\n"]}],"source":["#define training loop\n","for epoch in range(epochs):\n","    #shuffle data\n","    indices = np.arange(len(X_train))\n","    np.random.shuffle(indices)\n","    X_train = X_train[indices]\n","    y_train = y_train[indices]\n","    \n","    #go through data in batches\n","    for n in range(0, len(X_train), batch_size):\n","        #get batch data\n","        X_batch = X_train[n:n+batch_size]\n","        y_batch = y_train[n:n+batch_size]\n","        \n","        #perform forward pass\n","        dense1.forward(X_batch)\n","        relu1.forward(dense1.output)\n","        dense2.forward(relu1.output)\n","        relu2.forward(dense2.output)\n","        dense3.forward(relu2.output)\n","        #get loss\n","        loss = softmax_loss.forward(dense3.output, y_batch)\n","        \n","        #get accuracy\n","        predictions = np.argmax(softmax_loss.output, axis=1)\n","        accuracy = np.mean(predictions == y_batch)     \n","        \n","        #perform backpropagayion\n","        softmax_loss.backprop(softmax_loss.output, y_batch)\n","        dense3.backprop(softmax_loss.dinputs)\n","        relu2.backprop(dense3.dinputs)\n","        dense2.backprop(relu2.dinputs)\n","        relu1.backprop(dense2.dinputs)\n","        dense1.backprop(relu1.dinputs)\n","        \n","        #update weights and biases\n","        optimizer.update_params(dense1)\n","        optimizer.update_params(dense2)\n","        optimizer.update_params(dense3)\n","        optimizer.post_update_params()\n","    \n","    #display progress\n","    print(f'Epoch {epoch+1}/{epochs}, loss:{loss:.5f}, accuracy: {accuracy * 100:.3f}%')"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-08-14T17:13:29.319904Z","iopub.status.busy":"2023-08-14T17:13:29.318952Z","iopub.status.idle":"2023-08-14T17:13:29.423997Z","shell.execute_reply":"2023-08-14T17:13:29.422406Z","shell.execute_reply.started":"2023-08-14T17:13:29.319844Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Loss: 0.08551\n","Test Accuracy: 97.290%\n"]}],"source":["#perform forward pass on testing data\n","dense1.forward(X_test)\n","relu1.forward(dense1.output)\n","dense2.forward(relu1.output)\n","relu2.forward(dense2.output)\n","dense3.forward(relu2.output)\n","#calculate test loss\n","test_loss = softmax_loss.forward(dense3.output, y_test)\n","print(f'Test Loss: {test_loss:.5f}') \n","#calculate test accuracy\n","predictions = np.argmax(softmax_loss.output, axis=1) \n","accuracy = np.mean(predictions == y_test)\n","print(f'Test Accuracy: {accuracy * 100:.3f}%')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":4}
