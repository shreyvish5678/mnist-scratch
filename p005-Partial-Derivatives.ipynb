{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Backpropagation__\n",
    "\n",
    "What is backpropagation? Backpropagation is a process, where we go through the neural network backwards, in an effort to calculate the gradients of weights and biases. Then those gradients are used to fine tune to the weights and biases, to the minima of the loss function. A gradient quantifies the direction and rate of change of a function, which can be useful, in heading toWards the cost function minima. These gradients are calculated using calculus, specifically derivatives. Since our neural network is just a fancy function, we can use this strategy to calculate gradients, and decrease the cost function, making our model better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Partial Derivative__\n",
    "\n",
    "Our neuron function, consists of multiple inputs, such as the featurs, the weights and biases. Using the partial derivative, we can find out what as the impact of one weight or bias, on the entire cost function. So let's start by doing some partial derivatives. Consider a function $f(x, y) = x + y$. Let's calculate the partial derivative of $f(x, y)$ with respect to $x$ and $y$\n",
    "\n",
    "#### $\\frac{\\partial }{\\partial x}f(x,y) = \\frac{\\partial }{\\partial x}x + y = \\frac{\\partial }{\\partial x}x + \\frac{\\partial }{\\partial x}y = 1 + 0 = 1$\n",
    "#### $\\frac{\\partial }{\\partial y}f(x,y) = \\frac{\\partial }{\\partial y}x + y = \\frac{\\partial }{\\partial y}x + \\frac{\\partial }{\\partial y}y = 1 + 0 = 1$\n",
    "\n",
    "### __The partial derivative of a sum equals the sum of the partial derivatives__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try $f(x, y) = x * y$\n",
    "\n",
    "#### $\\frac{\\partial }{\\partial x}f(x,y) = \\frac{\\partial }{\\partial x}x * y = y\\frac{\\partial }{\\partial x}x$\n",
    "#### $\\frac{\\partial }{\\partial y}f(x,y) = \\frac{\\partial }{\\partial y}x * y = x\\frac{\\partial }{\\partial y}y$\n",
    "\n",
    "### __To calculate the partial derivative of multiplication, we move independent variables outside of the derivative and treat them like constants__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a harder example, with 3 variables\n",
    "\n",
    "#### $f(x, y, z) = 3x^3z - y^2 + 5z + 2yz$\n",
    "\n",
    "#### $\\frac{\\partial}{\\partial x}f(x, y, z) = \\frac{\\partial}{\\partial x}3x^3z - y^2 + 5z + 2yz = \\frac{\\partial}{\\partial x}3x^3z - \\frac{\\partial}{\\partial x}y^2 + \\frac{\\partial}{\\partial x}5z + \\frac{\\partial}{\\partial x}2yz = \\frac{\\partial}{\\partial x}3x^3z + 0 = z\\frac{\\partial}{\\partial x}3x^3 = 9x^2$\n",
    "\n",
    "#### $\\frac{\\partial}{\\partial y}f(x, y, z) = \\frac{\\partial}{\\partial y}3x^3z - y^2 + 5z + 2yz = \\frac{\\partial}{\\partial y}3x^3z - \\frac{\\partial}{\\partial y}y^2 + \\frac{\\partial}{\\partial y}5z + \\frac{\\partial}{\\partial y}2yz = \\frac{\\partial}{\\partial y}2yz - \\frac{\\partial}{\\partial y}y^2 = 2z - 2y$\n",
    "\n",
    "#### $\\frac{\\partial}{\\partial z}f(x, y, z) = \\frac{\\partial}{\\partial z}3x^3z - y^2 + 5z + 2yz = \\frac{\\partial}{\\partial z}3x^3z - \\frac{\\partial}{\\partial z}y^2 + \\frac{\\partial}{\\partial z}5z + \\frac{\\partial}{\\partial z}2yz = \\frac{\\partial}{\\partial y}3x^3z + \\frac{\\partial}{\\partial z}2yz + \\frac{\\partial}{\\partial z}5z = 3x^3 + 2y + 5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have looked at how partial derivative works for sum and multiplication, we need to look at the derivative of the ReLu function which is:\n",
    "\n",
    "#### $\\frac{d}{dx}max(0, x) = 1 (x > 0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Gradient__\n",
    "A gradient is a vector, that comprises of all of the partial derivatives of a function, with respect to each variable\n",
    "\n",
    "Here is the gradient for $f(x,y,z) = 3x^3z - y^2 + 5z + 2yz$:\n",
    "\n",
    "$\\nabla f(x, y, z) = \\begin{bmatrix}\n",
    "9xz^2 \\\\\n",
    "2z - 2y \\\\\n",
    "3x^3 +2y + 5 \\\\\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Chain Rule__\n",
    "Our loss function is just a chain of functions, such as softmax, relu and dense. To improve loss, we need to know how each weight and bias impacts it, and for a chain of functions, we can use the chain rule.\n",
    "\n",
    "#### $\\frac{d}{dx}f(g(x))=f'(g(x)) * g'(x)$\n",
    "\n",
    "To calculate the partial of a chain of function with respect to a variable, we take the partial derivative of the outermost function, with respect to the inner function. Then multiply this by the partial derivative the current inner function, and the next inner function, and repeat until you reach the variable.\n",
    "\n",
    "Let's do an example:\n",
    "\n",
    "$g(x) = 2x^2$\n",
    "\n",
    "$f(y) = 3y^5$\n",
    "\n",
    "#### $\\frac{d}{dx}f(g(x)) = f'(g(x)) * g'(x) = 15(2x^2)^4 * 4x = 240x^8 * 4x = 960x^9$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's for the math behind backpropagation, next we will be applying it, then adding it to our origial code. Let's keep going! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
