{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Activation Functions__\n",
    "\n",
    "First let's load in a dataset, and run our previous code on it, making a few slight changes, such as ading more layers and changing parameters. We will be importing the MNIST dataset, which is a dataset comprised of handwritten digits. Our project will be built around this dataset. We will use to load in our dataset. Then we will flatten each image array from 28x28 into a vector of size 784. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: (70000, 784)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('dataset.p', 'rb') as file:\n",
    "  X, y = pickle.load(file)\n",
    "\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "print(f'Input size: {X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 309364.5215605   211902.66957978 -208831.83333386 ... -315013.16757424\n",
      "   349481.74643741 -343205.23029616]\n",
      " [ 194721.61957701  307436.12271173  -79373.72638533 ...   43134.48232765\n",
      "   -76537.25584081 -344886.85772519]\n",
      " [ -64325.21569131  361371.95698315  -96925.75269489 ...  -33517.13392191\n",
      "   274963.39854676  -56024.38398058]\n",
      " ...\n",
      " [  28388.3853326   306543.52060894  160751.65440851 ...  -51462.80959851\n",
      "   161943.6190995   185346.88253138]\n",
      " [ -39453.54304542  162326.97457821  -69681.85804179 ... -252697.45713589\n",
      "   -63833.39513267  129302.80272159]\n",
      " [ 367982.6481871   166047.86945259 -321764.34322551 ... -521936.574629\n",
      "   134838.17629632  -25096.15075296]]\n"
     ]
    }
   ],
   "source": [
    "class Dense:\n",
    "  def __init__(self, input_neurons, output_neurons):\n",
    "    self.weights = np.random.randn(input_neurons, output_neurons)\n",
    "    self.biases = np.zeros((1, output_neurons))\n",
    "  def forward(self, inputs):\n",
    "    self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "layer_one = Dense(784, 128)\n",
    "layer_one.forward(X)\n",
    "layer_two = Dense(128, 64)\n",
    "layer_two.forward(layer_one.output)\n",
    "layer_three = Dense(64, 10)\n",
    "layer_three.forward(layer_two.output)\n",
    "print(layer_three.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's talk about activation functions. We are already using a linear activation function, $mx + b$ but generally we want to introduce non-linearity in our neural network, to be able to predict the data better. For this we will be using a simple activation function known as relu, which just takes negative values and converts them to 0, and leaves positive values alone. We will create a class relu, which just has a forward method.\n",
    "\n",
    "### Relu function: $f(x) = max(0, x)$\n",
    "\n",
    "Where $x$ is passed matrix. Now let's implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 17  0 14 68  0]\n"
     ]
    }
   ],
   "source": [
    "class ReLu:\n",
    "  def forward(self, inputs):\n",
    "    self.output = np.maximum(0, inputs)\n",
    "\n",
    "inputs = [-12, 17, -29, 14, 68, -7]\n",
    "relu_sample = ReLu()\n",
    "relu_sample.forward(inputs)\n",
    "print(relu_sample.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we passed a list with positive and negative values and it works! Now let's talk about the softmax activation. \n",
    "\n",
    "## __Softmax Activation__\n",
    "The softmax activation is important for being able to take our output from the neural network, and convert it into a probability distribution, where we can extract predicted labels. Let's assume we have a sample output, O. To apply softmax, we first subtract the biggest value in that sample, from all the values. This is to ensure when we apply exponents, there is no overflow. Next we apply the exponentiation, then we create a probability distribution by dividing all the values by the sum of the entire sample. \n",
    "\n",
    "### Softmax function: $f\\left(x\\right)=\\frac{e^{x_{i}-\\max\\left(x\\right)}}{\\sum_{n=1}^{k}e^{x_{n}-\\max\\left(x\\right)}}$\n",
    "\n",
    "where $k$ is the length of vector $x$. Now let's implement it!\n",
    "\n",
    "Note: We will use params, so we are only applying softmax to every sample differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bring together relu and softmax, to create a neural network! (Note: We still have loss, backpropagation and optimization to go, but the neural network architecture is fully created!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "relu_one = ReLu()\n",
    "relu_two = ReLu()\n",
    "softmax = Softmax()\n",
    "\n",
    "layer_one.forward(X)\n",
    "relu_one.forward(layer_one.output)\n",
    "layer_two.forward(relu_one.output)\n",
    "relu_two.forward(layer_two.output)\n",
    "layer_three.forward(relu_two.output)\n",
    "softmax.forward(layer_three.output)\n",
    "\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the relu and softmax activation worked. It created a probability distribution of all possible classes. But now we need to calculate the loss of this network, to see how good it is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
